{
    "model_type": "mistralai/Mistral-7B-v0.1",
    "model_name": "mistral-7b-sft",
    "load_lora": "",
    "saved_model_path": "./checkpoints/",

    "subtask_dir": "./natural-instructions-1.0/data/",
    "train_tasks": [
      "subtask035_winogrande_question_modification_person.json"
    ],
  
    "val_tasks": [
      "subtask040_qasc_question_generation.json"
    ],
  
    "template": "./templates/openai_natural_instructions_v1_template.txt",
    "pos_template": "./templates/natural_instructions_v1_positive_examples_template.txt",
    "neg_template": "./templates/natural_instructions_v1_negative_examples_template.txt",
    "openai_system_prompt": "./templates/openai_system_prompt.txt",
  
    "positive_examples": 2,
    "negative_examples": 2,
    "additional_instructions": true,

    "lora_rank": 64,
    "lora_alpha": 64,
    "lora_dropout": 0.05,
    "lora_modules": "q_proj,v_proj,k_proj,o_proj",
    "lora_bias": "lora_only",
  
    "max_input_length": 1900,
    "max_output_length": 128,  
    "max_context_length": 2048,
    "batch_size": 32,
    "eval_batch_size": 4,
    "gradient_accumulation_steps": 8,
    "weight_decay": 0.01,
    "max_lr": 0.00005,
    "min_lr": 0.000005,
    "cycle_steps": 5000,
    "warmup_steps": 500,
    "lr_gamma": 0.8,
  
    "num_epochs": 1,
    "eval_very_n_steps": 5,
  
    "dtype": "bfloat16",
    "compile": false,
    "flash_attn": true,
    "activation_checkpointing": true,
  
    "seed": 456970,
    "device": "cuda",
    "num_workers": 1,
    "tensorboard_log_dir": "./logs/",
  
    "do_sample": false,
    "temperature": 0.01,
    "top_p": 1.0,
    "repetition_penalty": 1.0
}